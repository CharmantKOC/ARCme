/**
 * Service OpenAI pour la g√©n√©ration de r√©ponses avec RAG
 * Utilise GPT-4 avec le contexte des documents trouv√©s
 */

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface GPTConfig {
  apiKey: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

export class OpenAIService {
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;

  constructor(config: GPTConfig) {
    this.apiKey = config.apiKey;
    this.model = config.model || 'gpt-4-turbo-preview';
    this.temperature = config.temperature ?? 0.7;
    this.maxTokens = config.maxTokens || 2000;
  }

  /**
   * G√©n√®re une r√©ponse avec streaming
   */
  async *generateResponseStream(
    messages: ChatMessage[]
  ): AsyncGenerator<string, void, unknown> {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: this.model,
          messages,
          temperature: this.temperature,
          max_tokens: this.maxTokens,
          stream: true
        })
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('No response body');

      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') return;

            try {
              const parsed = JSON.parse(data);
              const content = parsed.choices[0]?.delta?.content;
              if (content) {
                yield content;
              }
            } catch (e) {
              // Ignore parsing errors for incomplete JSON
            }
          }
        }
      }
    } catch (error) {
      console.error('Error in OpenAI stream:', error);
      throw error;
    }
  }

  /**
   * G√©n√®re une r√©ponse compl√®te (non-streaming)
   */
  async generateResponse(messages: ChatMessage[]): Promise<string> {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: this.model,
          messages,
          temperature: this.temperature,
          max_tokens: this.maxTokens
        })
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(`OpenAI API error: ${error.error?.message || response.statusText}`);
      }

      const data = await response.json();
      return data.choices[0]?.message?.content || '';
    } catch (error) {
      console.error('Error generating response:', error);
      throw error;
    }
  }

  /**
   * Cr√©e un prompt syst√®me pour le RAG
   */
  static createSystemPrompt(): string {
    return `Tu es un assistant IA sp√©cialis√© dans l'analyse de m√©moires acad√©miques de l'ARC (R√©seau des Anciens de l'Universit√©).

Ton r√¥le est d'aider les utilisateurs √† :
- Trouver des informations dans les m√©moires acad√©miques
- Comparer diff√©rentes approches m√©thodologiques
- Identifier les tendances et gaps de recherche
- Synth√©tiser les connaissances d'plusieurs documents

R√®gles importantes :
1. Base-toi UNIQUEMENT sur le contexte fourni (extraits de m√©moires)
2. Si l'information n'est pas dans le contexte, dis-le clairement
3. Cite toujours tes sources (titre du m√©moire, auteur)
4. Structure tes r√©ponses de mani√®re claire et acad√©mique
5. Utilise des emojis pour rendre la lecture agr√©able : üìö üìä üí° ‚úÖ ‚ö†Ô∏è
6. R√©ponds en fran√ßais

Format de r√©ponse id√©al :
üìö **Synth√®se** : [r√©ponse principale]
üìä **Sources** : [liste des m√©moires utilis√©s]
üí° **Insights** : [observations int√©ressantes]
‚û°Ô∏è **Pour aller plus loin** : [suggestions]`;
  }

  /**
   * Formate le contexte RAG pour le prompt
   */
  static formatRAGContext(results: Array<{
    content: string;
    metadata: {
      title: string;
      author: string;
      page_number?: number;
    };
    similarity: number;
  }>): string {
    if (results.length === 0) {
      return "Aucun document pertinent trouv√© dans la base de donn√©es.";
    }

    let context = `Voici ${results.length} extraits pertinents de m√©moires acad√©miques :\n\n`;
    
    results.forEach((result, idx) => {
      context += `[Document ${idx + 1}] "${result.metadata.title}" par ${result.metadata.author}`;
      if (result.metadata.page_number) {
        context += ` (page ${result.metadata.page_number})`;
      }
      context += `\nPertinence: ${(result.similarity * 100).toFixed(1)}%\n`;
      context += `Contenu:\n${result.content}\n\n`;
      context += "---\n\n";
    });

    return context;
  }
}

/**
 * Service singleton
 */
let openAIServiceInstance: OpenAIService | null = null;

export function getOpenAIService(): OpenAIService {
  const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
  
  if (!apiKey) {
    throw new Error('VITE_OPENAI_API_KEY not configured');
  }

  if (!openAIServiceInstance) {
    openAIServiceInstance = new OpenAIService({ apiKey });
  }

  return openAIServiceInstance;
}

export function isOpenAIConfigured(): boolean {
  return Boolean(import.meta.env.VITE_OPENAI_API_KEY);
}
